{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: John Murrowood\n",
    "#### Student ID: S3923075\n",
    "\n",
    "Date: 20/9/2022\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* sys\n",
    "* os\n",
    "* nltk\n",
    "* future\n",
    "* itertools\n",
    "\n",
    "## Introduction\n",
    "The following blocks of code will import a set of job advertisments. Only the webindex or these advertisments and their descriptions will be taken for processing from these documents. The aim is to then process the descriptions into list of tokens for each document to be easily processesd into a classification model to predict the job category that these jobs belong to. These jobs are first processed to ensure any html scraping inconsistencies are removed and then tokenised to only capture words that follow the particular format of the given regex expression. This will delete any symbols or expressions that could be errors or are not words. Other words are then deleted along the preprocessing process that are unnessisary for the model so it can be more accurate. This includes removing words with only one letter, removing stopwords that are irrelevant to job context, making all letetrs lowercase to avoid repeated words, and removing words that freuquently occurr and occur only once. \n",
    "\n",
    "The tokenised documents will then be saved into an output directory so they can be used later in classification models and a txt file of the vocabulary will also be saved so vector representations of the models can also be easily made in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported libraries used for task 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from __future__ import division\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code block reades in all the job txt files and extracts the webindex number of each job as well as the description.\n",
    "# The description of each job is saved as a list of strings in description_txt while the webindex is saved in description_ids.\n",
    "\n",
    "dir_path = \"./data\" # The directory path to access joadvertisments in data folder of given zip folder.\n",
    "description_ids = [] # list to store the advertisment webindex strings\n",
    "description_txts = [] # list to store the raw text of advertisment job descriptions\n",
    "webIndex_pattern = r\"Webindex: (\\d+)\" # Regex pattern used to extract webindex pattern\n",
    "for file in sorted(os.listdir(dir_path)): # Loop through each of the job category files to access all job advertisments\n",
    "    if file == \".DS_Store\": #Do not look in DS_Store file on mac\n",
    "        pass\n",
    "    else:\n",
    "        for filename in sorted(os.listdir((dir_path+\"/\"+file))): # load job advertisments in ascending order of txt file names\n",
    "            if filename.endswith(\".txt\"): # Ensure only txt files are opened\n",
    "                path = os.path.join(dir_path+\"/\"+file+\"/\"+filename) # This gives the path to the exact filename of each txt documents to open\n",
    "                with open(path,\"r\",encoding= 'unicode_escape') as f: # open the txt file\n",
    "                    text_whole = f.read() # Read in all the text in the advertisment\n",
    "                    description_ids.append(re.search(webIndex_pattern, text_whole).groups()[0]) # Extract the web index from the advertisent\n",
    "                                                                                                # and append to ids list\n",
    "                    text_description =  text_whole.split(\"\\nDescription: \")[1] # take only the job description text of the job advertisment\n",
    "                    description_txts.append(text_description) # Append description to list of description text for further processing\n",
    "                    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: Staff Nurse  RGN will also consider Newly Qualified Location: Selby Salary: **** per hour plus overtime rate Job Description: I am currently looking to recruit a qualified RGN to work for a service based within a rural location. The service is CQC compliant and part of a Yorkshire based Healthcare Company Job Requirements:Responsible for the assessment of care/support needs of service usersDevelopment and implementation of care programmes Working alongside other nurses reporting to the Manager Skills/ Qualifications:Registered Nurse  RGN will also consider newly qualified Desire to make a difference to people Passionate at delivering services that enhance lives Benefits:Salary **** per hour plus overtime rate Holiday entitlement EXCELLENT career progression and training opportunities Picturesque working environment For more information on how to apply for this fantastic opportunity please contact Shona Blackburn on or email a copy of your up to date CV for immediate attention to\n"
     ]
    }
   ],
   "source": [
    "# Remove any incorrect words with br or brbr infront of some of the words\n",
    "pattern = r\"(?:br{1,2})[A-Z]\\w*\" # Pattern for extracting unwanted br or brbr prefixes to some words\n",
    "for a in range(len(description_txts)): # Loop through advertisment descriptions\n",
    "    i = description_txts[a] # load in one description at a time for checking\n",
    "    if re.findall(pattern, i) != []: # If brbr not found no need to do any fixing\n",
    "        for j in re.findall(pattern, i): # Loop through each found instance of br or brbr\n",
    "            i = i.replace(j, j[2:]) # If br prefix then remove\n",
    "            if 'br' in i: # If second br still in word\n",
    "                i = i.replace(j, j[2:]) # Remove second instance of br prefix\n",
    "        description_txts[a] = i # Add fixed text body back and replace old body\n",
    "        print(i) # Check to ensure fix has worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of 68559978\n",
      "['Position', 'Staff', 'Nurse', 'RGN', 'will', 'also', 'consider', 'Newly', 'Qualified', 'Location', 'Selby', 'Salary', 'per', 'hour', 'plus', 'overtime', 'rate', 'Job', 'Description', 'I', 'am', 'currently', 'looking', 'to', 'recruit', 'a', 'qualified', 'RGN', 'to', 'work', 'for', 'a', 'service', 'based', 'within', 'a', 'rural', 'location', 'The', 'service', 'is', 'CQC', 'compliant', 'and', 'part', 'of', 'a', 'Yorkshire', 'based', 'Healthcare', 'Company', 'Job', 'Requirements', 'Responsible', 'for', 'the', 'assessment', 'of', 'care', 'support', 'needs', 'of', 'service', 'usersDevelopment', 'and', 'implementation', 'of', 'care', 'programmes', 'Working', 'alongside', 'other', 'nurses', 'reporting', 'to', 'the', 'Manager', 'Skills', 'Qualifications', 'Registered', 'Nurse', 'RGN', 'will', 'also', 'consider', 'newly', 'qualified', 'Desire', 'to', 'make', 'a', 'difference', 'to', 'people', 'Passionate', 'at', 'delivering', 'services', 'that', 'enhance', 'lives', 'Benefits', 'Salary', 'per', 'hour', 'plus', 'overtime', 'rate', 'Holiday', 'entitlement', 'EXCELLENT', 'career', 'progression', 'and', 'training', 'opportunities', 'Picturesque', 'working', 'environment', 'For', 'more', 'information', 'on', 'how', 'to', 'apply', 'for', 'this', 'fantastic', 'opportunity', 'please', 'contact', 'Shona', 'Blackburn', 'on', 'or', 'email', 'a', 'copy', 'of', 'your', 'up', 'to', 'date', 'CV', 'for', 'immediate', 'attention', 'to']\n",
      "Number of tokens found in job description:  229\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation using given regular expression:\n",
    "pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\" # Given regex expression to perform tokenisation\n",
    "description_txt_token = [] # Establish a list of lists of tokenised words in each description\n",
    "tokenizer = RegexpTokenizer(pattern) # Initialise tokeniser using regex pattern\n",
    "for desc in description_txts: # Loop through all desciptions\n",
    "    tokens = tokenizer.tokenize(desc) # Get tokens from each job description\n",
    "    description_txt_token.append(tokens) # Append list of tokens for each document to tokenised documents list\n",
    "\n",
    "# Print out random document to check tokenisation worked\n",
    "print(\"Description of\", description_ids[488]) \n",
    "print(description_txt_token[488])\n",
    "print(\"Number of tokens found in job description: \", len(description_txt_token[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of 70757636\n",
      "['you', 'will', 'be', 'responsible', 'for', 'the', 'efficient', 'running', 'of', 'the', 'accounting', 'function', 'of', 'the', 'business', 'by', 'implementing', 'procedures', 'and', 'controls', 'to', 'ensure', 'the', 'accuracy', 'of', 'the', 'management', 'information', 'presented', 'to', 'the', 'directors', 'to', 'establish', 'and', 'maintain', 'financial', 'policies', 'and', 'management', 'information', 'systems', 'and', 'to', 'liaise', 'with', 'management', 'colleagues', 'on', 'all', 'aspects', 'of', 'finance', 'to', 'provide', 'a', 'high', 'quality', 'support', 'service', 'to', 'the', 'directors', 'and', 'officers', 'of', 'the', 'company', 'duties', 'to', 'include', 'leading', 'a', 'team', 'of', 'staff', 'responsible', 'for', 'the', 'production', 'of', 'management', 'accounts', 'creating', 'implementing', 'and', 'monitoring', 'processes', 'and', 'procedures', 'for', 'the', 'creation', 'of', 'monthly', 'forecasts', 'preparing', 'detailed', 'weekly', 'monthly', 'and', 'quarterly', 'cash', 'flow', 'forecasts', 'preparing', 'detailed', 'annual', 'quarterly', 'and', 'monthly', 'profit', 'forecasts', 'preparing', 'detailed', 'monthly', 'financial', 'statements', 'to', 'include', 'profit', 'and', 'loss', 'accounts', 'balance', 'sheet', 'variance', 'analysis', 'and', 'commentaries', 'within', 'working', 'days', 'of', 'month', 'end', 'managing', 'income', 'and', 'expenditure', 'sales', 'payroll', 'and', 'stocks', 'offering', 'professional', 'judgement', 'on', 'financial', 'matters', 'and', 'advising', 'on', 'ways', 'of', 'improving', 'business', 'performance', 'carrying', 'out', 'any', 'necessary', 'research', 'analysing', 'it', 'and', 'concluding', 'on', 'the', 'information', 'liaising', 'with', 'other', 'managers', 'to', 'put', 'the', 'finance', 'view', 'in', 'context', 'safeguarding', 'tangible', 'and', 'intangible', 'assets', 'any', 'other', 'duties', 'of', 'a', 'financial', 'matter', 'that', 'occur', 'from', 'time', 'to', 'time', 'preparation', 'of', 'yearend', 'file', 'for', 'independent', 'accountants', 'to', 'produce', 'yearly', 'financial', 'statements', 'for', 'hm', 'revenue', 'customs', 'and', 'companies', 'house', 'purposes', 'cima', 'or', 'acca', 'qualified', 'ideally', 'with', 'background', 'within', 'retail', 'industry']\n",
      "Number of tokens found in job description:  229\n"
     ]
    }
   ],
   "source": [
    "# convert all tokens to lower case:\n",
    "description_txt_token_lower = [] # Initialise a list for all lower case tokens\n",
    "for desc in description_txt_token: # loop through each tokenised desription\n",
    "    for i in range(len(desc)): # loop through each token\n",
    "        desc[i] = desc[i].lower() # Force every word to be all lower case\n",
    "    description_txt_token_lower.append(desc) # Append list of lowercase tokens to lowered tokens list\n",
    "\n",
    "# Print out the lowered tokens to ensure it has worked\n",
    "print(\"Description of\", description_ids[488])\n",
    "print(description_txt_token_lower[488])\n",
    "print(\"Number of tokens found in job description: \", len(description_txt_token_lower[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of 70757636\n",
      "['you', 'will', 'be', 'responsible', 'for', 'the', 'efficient', 'running', 'of', 'the', 'accounting', 'function', 'of', 'the', 'business', 'by', 'implementing', 'procedures', 'and', 'controls', 'to', 'ensure', 'the', 'accuracy', 'of', 'the', 'management', 'information', 'presented', 'to', 'the', 'directors', 'to', 'establish', 'and', 'maintain', 'financial', 'policies', 'and', 'management', 'information', 'systems', 'and', 'to', 'liaise', 'with', 'management', 'colleagues', 'on', 'all', 'aspects', 'of', 'finance', 'to', 'provide', 'high', 'quality', 'support', 'service', 'to', 'the', 'directors', 'and', 'officers', 'of', 'the', 'company', 'duties', 'to', 'include', 'leading', 'team', 'of', 'staff', 'responsible', 'for', 'the', 'production', 'of', 'management', 'accounts', 'creating', 'implementing', 'and', 'monitoring', 'processes', 'and', 'procedures', 'for', 'the', 'creation', 'of', 'monthly', 'forecasts', 'preparing', 'detailed', 'weekly', 'monthly', 'and', 'quarterly', 'cash', 'flow', 'forecasts', 'preparing', 'detailed', 'annual', 'quarterly', 'and', 'monthly', 'profit', 'forecasts', 'preparing', 'detailed', 'monthly', 'financial', 'statements', 'to', 'include', 'profit', 'and', 'loss', 'accounts', 'balance', 'sheet', 'variance', 'analysis', 'and', 'commentaries', 'within', 'working', 'days', 'of', 'month', 'end', 'managing', 'income', 'and', 'expenditure', 'sales', 'payroll', 'and', 'stocks', 'offering', 'professional', 'judgement', 'on', 'financial', 'matters', 'and', 'advising', 'on', 'ways', 'of', 'improving', 'business', 'performance', 'carrying', 'out', 'any', 'necessary', 'research', 'analysing', 'it', 'and', 'concluding', 'on', 'the', 'information', 'liaising', 'with', 'other', 'managers', 'to', 'put', 'the', 'finance', 'view', 'in', 'context', 'safeguarding', 'tangible', 'and', 'intangible', 'assets', 'any', 'other', 'duties', 'of', 'financial', 'matter', 'that', 'occur', 'from', 'time', 'to', 'time', 'preparation', 'of', 'yearend', 'file', 'for', 'independent', 'accountants', 'to', 'produce', 'yearly', 'financial', 'statements', 'for', 'hm', 'revenue', 'customs', 'and', 'companies', 'house', 'purposes', 'cima', 'or', 'acca', 'qualified', 'ideally', 'with', 'background', 'within', 'retail', 'industry']\n",
      "Number of tokens found in job description with initial removal:  226\n"
     ]
    }
   ],
   "source": [
    "# Remove words with length less than 2:\n",
    "min_word_length = 2 # Initialise the minimum word length \n",
    "description_txt_token_lower_rem = [] # Initialise list to add new tokens with words removed\n",
    "for desc in description_txt_token_lower: # Loop through each advertisment tokenised list\n",
    "    for word in desc: # Loop through each token\n",
    "        if len(word) < min_word_length: # Check if word is less than the minimum length\n",
    "            desc.remove(word)\n",
    "    description_txt_token_lower_rem.append(desc)\n",
    "\n",
    "# Check removal worked\n",
    "print(\"Description of\", description_ids[1])\n",
    "print(description_txt_token_lower_rem[1])\n",
    "print(\"Number of tokens found in job description with initial removal: \", len(description_txt_token_lower_rem[1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stopwords file\n",
    "stop_file = \"stopwords_en.txt\"\n",
    "with open(stop_file, \"r\", encoding= 'unicode_escape') as f: # open the txt file\n",
    "    stop_words = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of 70757636\n",
      "['responsible', 'efficient', 'running', 'accounting', 'function', 'business', 'implementing', 'procedures', 'controls', 'ensure', 'accuracy', 'management', 'information', 'presented', 'directors', 'establish', 'maintain', 'financial', 'policies', 'management', 'information', 'systems', 'liaise', 'management', 'colleagues', 'aspects', 'finance', 'provide', 'high', 'quality', 'support', 'service', 'directors', 'officers', 'company', 'duties', 'include', 'leading', 'team', 'staff', 'responsible', 'production', 'management', 'accounts', 'creating', 'implementing', 'monitoring', 'processes', 'procedures', 'creation', 'monthly', 'forecasts', 'preparing', 'detailed', 'weekly', 'monthly', 'quarterly', 'cash', 'flow', 'forecasts', 'preparing', 'detailed', 'annual', 'quarterly', 'monthly', 'profit', 'forecasts', 'preparing', 'detailed', 'monthly', 'financial', 'statements', 'include', 'profit', 'loss', 'accounts', 'balance', 'sheet', 'variance', 'analysis', 'commentaries', 'working', 'days', 'month', 'end', 'managing', 'income', 'expenditure', 'sales', 'payroll', 'stocks', 'offering', 'professional', 'judgement', 'financial', 'matters', 'advising', 'ways', 'improving', 'business', 'performance', 'carrying', 'research', 'analysing', 'concluding', 'information', 'liaising', 'managers', 'put', 'finance', 'view', 'context', 'safeguarding', 'tangible', 'intangible', 'assets', 'duties', 'financial', 'matter', 'occur', 'time', 'time', 'preparation', 'yearend', 'file', 'independent', 'accountants', 'produce', 'yearly', 'financial', 'statements', 'hm', 'revenue', 'customs', 'companies', 'house', 'purposes', 'cima', 'acca', 'qualified', 'ideally', 'background', 'retail', 'industry']\n",
      "Number of tokens found in job description with initial removal:  144\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords\n",
    "description_txt_token_no_stop = [] # Initialise tokens list with stopwords removed\n",
    "for desc in description_txt_token_lower_rem: # Loop through existing tokens\n",
    "    description_txt_token_no_stop.append([t for t in desc if t not in stop_words.split(\"\\n\")]) # Add tokens to new list if they are not a stop word\n",
    "# Check if stopword removal worked\n",
    "print(\"Description of\", description_ids[1])\n",
    "print(description_txt_token_no_stop[1])\n",
    "print(\"Number of tokens found in job description with initial removal: \", len(description_txt_token_no_stop[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a term and doc frequency to remove low frequency and high frequncy words\n",
    "words = list(chain.from_iterable(description_txt_token_no_stop))\n",
    "words_2 = list(chain.from_iterable([set(description) for description in description_txt_token_no_stop]))\n",
    "doc_fd = FreqDist(words_2) # compute term frequency for each unique word/type\n",
    "term_fd = FreqDist(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words that appear only once\n",
    "lessFreqWords = set(term_fd.hapaxes())\n",
    "description_txt_token_freq = [] # Initilise list for filtered tokens\n",
    "for desc in description_txt_token_no_stop:\n",
    "    description_txt_token_freq.append([t for t in desc if t not in lessFreqWords]) # Add words to list that appear more than once only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove top 50 most occuring words\n",
    "moreFreqwords = doc_fd.most_common(50)\n",
    "most_words = []\n",
    "for i in moreFreqwords:\n",
    "    most_words.append(i[0])\n",
    "\n",
    "for i in range(len(description_txt_token_no_stop)):\n",
    "    description_txt_token_freq[i] = [t for t in description_txt_token_freq[i] if t not in most_words] # Delete words that are in top 50 most occuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['description', 'inspired', 'selections', 'optical', 'assistant', 'oxfordshire', 'locally', 'level', 'customer', 'established', 'years', 'ago', 'friendly', 'professional', 'highly', 'competitive', 'prices', 'practices', 'fully', 'qualified', 'dispensing', 'opticians', 'hand', 'members', 'association', 'british', 'dispensing', 'opticians', 'small', 'independent', 'group', 'prices', 'larger', 'chains', 'patients', 'enjoy', 'worlds', 'committed', 'care', 'staff', 'respected', 'loyal', 'patients', 'inspired', 'selections']\n"
     ]
    }
   ],
   "source": [
    "# Check that word filter worked\n",
    "print(description_txt_token_freq[478])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "- vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving job advertisement outputs to text files for each ad\n",
    "os.mkdir('output/') # Make a new output directory to store tokenised txt files\n",
    "for i in range(len(description_ids)): # loop through each tokenised list\n",
    "    with open('output/'+description_ids[i], 'w') as output:\n",
    "        output.write(' '.join([str(elem) for elem in description_txt_token_freq[i]])) #Write each tokenised word seperated by a space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving vocab txt file with all stopwords removed as specified\n",
    "file_name = 'vocab.txt'\n",
    "words = sorted(list(chain.from_iterable(description_txt_token_freq))) #Make each word appear in alphabetical order\n",
    "vocab = list(dict.fromkeys(words)) # Make vocab list \n",
    "with open(file_name, 'w') as f:\n",
    "    for i in range(len(vocab)):\n",
    "        f.write(f\"{vocab[i]}:{i}\\n\") # Save each word with index on txt file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "The vocab file was able to be outputted in the correct formnat of word:index and the tokenised descriptions were each saved to their own txt file in a directory called output. Each txt file was given the name of its webindex so the webindex can easily be matched to the description for future reference. It could be seen that removing stop words removed a large portion of the tokenised words showing this is a very important step because these words have very little context on the point of the description and therefore arent helpful in the classification model. The regex pattern was also very effective at ensuring only words were captured with no numbers or other symbols prersent in some of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('casestudies')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "02dd39696e8b9a6a290c5d62f8b7bc0675cf861429cf5c1fa190f1f9fbbf3d64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
